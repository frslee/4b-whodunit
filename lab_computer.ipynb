{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#First-Visit\" data-toc-modified-id=\"First-Visit-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>First Visit</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gathering-Information-on-Addresses-to-interview-the-person-who-left-before-the-gunshots-were-heard!\" data-toc-modified-id=\"Gathering-Information-on-Addresses-to-interview-the-person-who-left-before-the-gunshots-were-heard!-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Gathering Information on Addresses to interview the person who left before the gunshots were heard!</a></span></li><li><span><a href=\"#Time-for-more-investigation!\" data-toc-modified-id=\"Time-for-more-investigation!-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Time for more investigation!</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-search-for-filenames-in-Linux-dir-matching-string:\" data-toc-modified-id=\"How-to-search-for-filenames-in-Linux-dir-matching-string:-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>How to search for filenames in Linux dir matching string:</a></span></li></ul></li></ul></li><li><span><a href=\"#Second-Visit\" data-toc-modified-id=\"Second-Visit-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Second Visit</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-out-the-interviews\" data-toc-modified-id=\"Checking-out-the-interviews-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Checking out the interviews</a></span></li><li><span><a href=\"#Checking-the-Vehicle-Database\" data-toc-modified-id=\"Checking-the-Vehicle-Database-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Checking the Vehicle Database</a></span></li><li><span><a href=\"#Gathering-Information-on-Memberships\" data-toc-modified-id=\"Gathering-Information-on-Memberships-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Gathering Information on Memberships</a></span></li><li><span><a href=\"#Putting-it-all-together\" data-toc-modified-id=\"Putting-it-all-together-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Putting it all together</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Police Station Lab Computer\n",
    "\n",
    "Let's use the computer, and the data it contains, to see if we can't crack this mystery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Information on Addresses to interview the person who left before the gunshots were heard!\n",
    "\n",
    "First thing to do, is load up the people `.csv` - we'll notice that it's separated by tabs (`\\t`) and there's some comments (`#`), so let's take those into consideration. \n",
    "\n",
    "Then, let's look at the `.head()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alicia Fuentes</td>\n",
       "      <td>F</td>\n",
       "      <td>48</td>\n",
       "      <td>Walton Street, line 433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jo-Ting Losev</td>\n",
       "      <td>F</td>\n",
       "      <td>46</td>\n",
       "      <td>Hemenway Street, line 390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elena Edmonds</td>\n",
       "      <td>F</td>\n",
       "      <td>58</td>\n",
       "      <td>Elmwood Avenue, line 123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naydene Cabral</td>\n",
       "      <td>F</td>\n",
       "      <td>46</td>\n",
       "      <td>Winthrop Street, line 454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dato Rosengren</td>\n",
       "      <td>M</td>\n",
       "      <td>22</td>\n",
       "      <td>Mystic Street, line 477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name gender  age                    address\n",
       "0  Alicia Fuentes      F   48    Walton Street, line 433\n",
       "1   Jo-Ting Losev      F   46  Hemenway Street, line 390\n",
       "2   Elena Edmonds      F   58   Elmwood Avenue, line 123\n",
       "3  Naydene Cabral      F   46  Winthrop Street, line 454\n",
       "4  Dato Rosengren      M   22    Mystic Street, line 477"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "people_dataframe = pd.read_csv('./data/people/people.csv', comment='#', sep='\\t')\n",
    "people_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some indexing to find out if we can narrow this list down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Annabel Sun</td>\n",
       "      <td>F</td>\n",
       "      <td>26</td>\n",
       "      <td>Hart Place, line 40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>Annabel Church</td>\n",
       "      <td>F</td>\n",
       "      <td>38</td>\n",
       "      <td>Buckingham Place, line 179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name gender  age                     address\n",
       "269      Annabel Sun      F   26         Hart Place, line 40\n",
       "1661  Annabel Church      F   38  Buckingham Place, line 179"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# be sure to include the information in the format provided from the DataFrame above!\n",
    "potential_interviewees = people_dataframe[(people_dataframe['name'].str.contains('Annabel')) & (people_dataframe['gender'] == 'F')]\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "potential_interviewees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Annabel Sun</td>\n",
       "      <td>F</td>\n",
       "      <td>26</td>\n",
       "      <td>Hart Place, line 40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>Annabel Church</td>\n",
       "      <td>F</td>\n",
       "      <td>38</td>\n",
       "      <td>Buckingham Place, line 179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name gender  age                     address\n",
       "269      Annabel Sun      F   26         Hart Place, line 40\n",
       "1661  Annabel Church      F   38  Buckingham Place, line 179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_interviewees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for more investigation!\n",
    "\n",
    "You'll need to hit the streets before you can pull up any relevant interviews, come back when you've knocked on the doors on the addresses you found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbot_Street\t\t  Winter_Street       interview-48148020\r\n",
      "Acton_Street\t\t  Woodcliff_Street    interview-483817\r\n",
      "Addington_Road\t\t  Wooddale_Avenue     interview-485229\r\n",
      "Alaric_Street\t\t  Worrell_Street      interview-4950099\r\n",
      "Albany_Street\t\t  Wycliff_Avenue      interview-4961376\r\n",
      "Aldworth_Street\t\t  crimescene\t      interview-496772\r\n",
      "Alpine_Street\t\t  data\t\t      interview-498331\r\n",
      "Andover_Road\t\t  helper_functions    interview-499096\r\n",
      "Ansonia_Road\t\t  hint\t\t      interview-50168425\r\n",
      "Appleton_Road\t\t  instructions\t      interview-50291987\r\n",
      "Aramon_Street\t\t  interview-000296    interview-504687\r\n",
      "Arcola_Street\t\t  interview-00448418  interview-509105\r\n",
      "Atwood_Road\t\t  interview-00502304  interview-5143029\r\n",
      "Auriga_Street\t\t  interview-005702    interview-514793\r\n",
      "Avalon_Road\t\t  interview-00617019  interview-52280505\r\n",
      "B_Street\t\t  interview-00805135  interview-528044\r\n",
      "Ballard_Street\t\t  interview-016463    interview-529706\r\n",
      "Balmoral_Park\t\t  interview-020337    interview-53318557\r\n",
      "Bartlett_Avenue\t\t  interview-022751    interview-535181\r\n",
      "Bellevue_Avenue\t\t  interview-0234126   interview-5372865\r\n",
      "Bertson_Avenue\t\t  interview-02422821  interview-538900\r\n",
      "Bicknell_Street\t\t  interview-0251720   interview-54026669\r\n",
      "Blue_Hill_Avenue\t  interview-03098229  interview-541518\r\n",
      "Bobolink_Street\t\t  interview-0315125   interview-5455315\r\n",
      "Boston_Street\t\t  interview-03316077  interview-54619323\r\n",
      "Bothwell_Road\t\t  interview-034070    interview-54851634\r\n",
      "Bournedale_Road\t\t  interview-0349327   interview-549055\r\n",
      "Boynton_Street\t\t  interview-04393507  interview-55382746\r\n",
      "Brinton_Street\t\t  interview-044492    interview-55410365\r\n",
      "Bristol_Street\t\t  interview-0462097   interview-55435298\r\n",
      "Broad_Canal_Street\t  interview-049721    interview-55477243\r\n",
      "Brookdale_Street\t  interview-05297663  interview-555536\r\n",
      "Brookline_Avenue\t  interview-06032377  interview-5581158\r\n",
      "Buckingham_Place\t  interview-0613334   interview-55841398\r\n",
      "Burwell_Road\t\t  interview-066291    interview-55984022\r\n",
      "Cadbury_Road\t\t  interview-071537    interview-565396\r\n",
      "Cardinal_Medeiros_Avenue  interview-0732631   interview-566707\r\n",
      "Cardington_Street\t  interview-07497003  interview-56784802\r\n",
      "Causeway_Street\t\t  interview-0768255   interview-56892213\r\n",
      "Cerdan_Avenue\t\t  interview-092423    interview-57236791\r\n",
      "Channel_Center_Street\t  interview-0953437   interview-5739404\r\n",
      "Cheverus_Road\t\t  interview-096267    interview-5766907\r\n",
      "Claremont_Park\t\t  interview-102490    interview-5774468\r\n",
      "Clay_Street\t\t  interview-109118    interview-5782759\r\n",
      "Claybourne_Street\t  interview-1108561   interview-579105\r\n",
      "Clipper_Ship_Lane\t  interview-114661    interview-5835471\r\n",
      "Cook_Street\t\t  interview-11495001  interview-586668\r\n",
      "Cornelia_Street\t\t  interview-116803    interview-58910793\r\n",
      "Corwin_Street\t\t  interview-11705111  interview-5905106\r\n",
      "Culbert_Street\t\t  interview-11783660  interview-591273\r\n",
      "Dacia_Street\t\t  interview-11817172  interview-5993978\r\n",
      "Dalton_Street\t\t  interview-1186827   interview-60081985\r\n",
      "Dana_Avenue\t\t  interview-1205060   interview-604403\r\n",
      "Davenport_Street\t  interview-1250176   interview-608607\r\n",
      "Devine_Way\t\t  interview-125204    interview-6093093\r\n",
      "Dewar_Street\t\t  interview-125271    interview-618764\r\n",
      "Doane_Street\t\t  interview-1269181   interview-6203192\r\n",
      "Dock_Street\t\t  interview-1310392   interview-628618\r\n",
      "Dorrance_Street\t\t  interview-13768464  interview-63308519\r\n",
      "Dover_Street\t\t  interview-13889608  interview-637657\r\n",
      "Dresser_Street\t\t  interview-13920860  interview-637928\r\n",
      "Drumlin_Road\t\t  interview-1395414   interview-638121\r\n",
      "Dunstable_Road\t\t  interview-141030    interview-6417794\r\n",
      "Elmore_Street\t\t  interview-14153840  interview-645385\r\n",
      "Embassy_Road\t\t  interview-144873    interview-6553472\r\n",
      "Enterprise_Street\t  interview-14590717  interview-65792229\r\n",
      "Esther_Road\t\t  interview-147283    interview-659803\r\n",
      "Estrella_Street\t\t  interview-15187437  interview-66101490\r\n",
      "Eugenia_Road\t\t  interview-15354942  interview-66282920\r\n",
      "Fairfield_Street\t  interview-1536668   interview-6643191\r\n",
      "Faunce_Road\t\t  interview-155049    interview-67279454\r\n",
      "Fay_Street\t\t  interview-1578206   interview-673985\r\n",
      "Fayston_Street\t\t  interview-159848    interview-676473\r\n",
      "Fessenden_Street\t  interview-16098538  interview-67790846\r\n",
      "Forestvale_Road\t\t  interview-1642421   interview-680549\r\n",
      "Foundry_Street\t\t  interview-1643440   interview-6808205\r\n",
      "Fowler_Street\t\t  interview-16889008  interview-68195573\r\n",
      "Frontage_Road\t\t  interview-17248453  interview-68488577\r\n",
      "Gerard_Street\t\t  interview-17343208  interview-68764140\r\n",
      "Gillespies_Lane\t\t  interview-174898    interview-6884359\r\n",
      "Glenville_Avenue\t  interview-1767435   interview-6894000\r\n",
      "Glenwood_Avenue\t\t  interview-17827186  interview-69170457\r\n",
      "Grampian_Way\t\t  interview-179719    interview-6933068\r\n",
      "Granby_Street\t\t  interview-1811770   interview-699607\r\n",
      "Gray_Gardens_East\t  interview-18193261  interview-70067280\r\n",
      "Greendale_Road\t\t  interview-1823688   interview-70199425\r\n",
      "Gretter_Road\t\t  interview-18270219  interview-703831\r\n",
      "Groveland_Street\t  interview-18441251  interview-704443\r\n",
      "Hadassah_Way\t\t  interview-1850922   interview-70458099\r\n",
      "Haley_Street\t\t  interview-1857368   interview-7046684\r\n",
      "Harbor_Point_Boulevard\t  interview-1906958   interview-7066082\r\n",
      "Harding_Road\t\t  interview-191206    interview-706620\r\n",
      "Hart_Place\t\t  interview-19300543  interview-707438\r\n",
      "Hazel_Street\t\t  interview-1933118   interview-708943\r\n",
      "Hazelton_Street\t\t  interview-19577850  interview-7103823\r\n",
      "High_Road\t\t  interview-2058907   interview-71186817\r\n",
      "High_View_Avenue\t  interview-210355    interview-71226767\r\n",
      "Hobart_Street\t\t  interview-218131    interview-71298441\r\n",
      "Hollander_Street\t  interview-221039    interview-7180973\r\n",
      "Hooten_Court\t\t  interview-223913    interview-71993338\r\n",
      "Hopedale_Street\t\t  interview-2277882   interview-720268\r\n",
      "Hull_Street\t\t  interview-229443    interview-7254073\r\n",
      "Island_View_Place\t  interview-23167806  interview-728181\r\n",
      "Jacob_Street\t\t  interview-2326746   interview-730123\r\n",
      "Jacqueline_Road\t\t  interview-23371263  interview-73035802\r\n",
      "Jamaica_Place\t\t  interview-233800    interview-7305678\r\n",
      "Jersey_Street\t\t  interview-2415821   interview-73585672\r\n",
      "Jeshurun_Street\t\t  interview-243703    interview-737609\r\n",
      "John_Alden_Road\t\t  interview-2481877   interview-7422077\r\n",
      "Judge_Street\t\t  interview-250112    interview-74225310\r\n",
      "June_Street\t\t  interview-253705    interview-7469675\r\n",
      "Kearsarge_Avenue\t  interview-255531    interview-7541406\r\n",
      "King_Place\t\t  interview-25582311  interview-75434722\r\n",
      "Kinross_Road\t\t  interview-25834905  interview-755037\r\n",
      "Knoll_Street\t\t  interview-259909    interview-75633580\r\n",
      "LICENSE\t\t\t  interview-2601508   interview-7580872\r\n",
      "Laban_Pratt_Road\t  interview-26373485  interview-77014856\r\n",
      "Leseur_Road\t\t  interview-2642139   interview-770439\r\n",
      "Lineham_Court\t\t  interview-27042476  interview-77135281\r\n",
      "Lovis_Street\t\t  interview-27504937  interview-7791374\r\n",
      "Lynde_Street\t\t  interview-275706    interview-780255\r\n",
      "Lynn_Street\t\t  interview-279087    interview-7863761\r\n",
      "Mamelon_Circle\t\t  interview-280877    interview-789564\r\n",
      "Manila_Avenue\t\t  interview-2834518   interview-791289\r\n",
      "Manton_Terrace\t\t  interview-284560    interview-79360358\r\n",
      "Marney_Street\t\t  interview-2846076   interview-79411932\r\n",
      "Mattapan_Street\t\t  interview-289524    interview-794525\r\n",
      "May_Street\t\t  interview-290346    interview-7959148\r\n",
      "Meadowview_Road\t\t  interview-291440    interview-796439\r\n",
      "Merola_Park\t\t  interview-2922290   interview-79667499\r\n",
      "Michigan_Avenue\t\t  interview-29316965  interview-79935965\r\n",
      "Miles_Street\t\t  interview-2939888   interview-7998181\r\n",
      "Moloney_Street\t\t  interview-296128    interview-8095917\r\n",
      "Mount_Ash_Road\t\t  interview-29680692  interview-809922\r\n",
      "Mount_Ida_Road\t\t  interview-29741223  interview-812725\r\n",
      "Mountain_Avenue\t\t  interview-2976680   interview-81443363\r\n",
      "Mozart_Street\t\t  interview-29838622  interview-822576\r\n",
      "Myles_Standish_Road\t  interview-2995681   interview-8245680\r\n",
      "Mystic_Street\t\t  interview-301018    interview-825165\r\n",
      "Nazing_Street\t\t  interview-30259493  interview-82705993\r\n",
      "New_Minton_Street\t  interview-3049045   interview-831512\r\n",
      "New_Park_Avenue\t\t  interview-305694    interview-833367\r\n",
      "Newcastle_Road\t\t  interview-305949    interview-838259\r\n",
      "Newcroft_Circle\t\t  interview-306616    interview-8387710\r\n",
      "North_Washington_Street   interview-3074127   interview-8402388\r\n",
      "Norton_Street\t\t  interview-3099757   interview-8421696\r\n",
      "Oak_Hill_Avenue\t\t  interview-312546    interview-8464899\r\n",
      "Oak_Place\t\t  interview-3128999   interview-84688694\r\n",
      "Oakley_Street\t\t  interview-3140662   interview-849256\r\n",
      "Oliva_Road\t\t  interview-31635890  interview-85262552\r\n",
      "Orchard_Road\t\t  interview-3201508   interview-8531248\r\n",
      "Paragon_Road\t\t  interview-322305    interview-856221\r\n",
      "Peacock_Lane\t\t  interview-32365018  interview-8586380\r\n",
      "Peter_Parley_Road\t  interview-324389    interview-861780\r\n",
      "Pinewood_Street\t\t  interview-325611    interview-862173\r\n",
      "Plain_Street\t\t  interview-32639981  interview-862717\r\n",
      "Plainfield_Street\t  interview-32712166  interview-8631232\r\n",
      "Potosi_Street\t\t  interview-331178    interview-86395001\r\n",
      "Priscilla_Road\t\t  interview-332596    interview-865918\r\n",
      "Proctor_Street\t\t  interview-33399976  interview-867999\r\n",
      "Quarley_Road\t\t  interview-340396    interview-8700943\r\n",
      "README.md\t\t  interview-34041151  interview-87126591\r\n",
      "Randlett_Place\t\t  interview-342393    interview-871877\r\n",
      "Rena_Street\t\t  interview-34359897  interview-879569\r\n",
      "Renfrew_Street\t\t  interview-344331    interview-8819490\r\n",
      "Richard_Avenue\t\t  interview-34690644  interview-891720\r\n",
      "Richardson_Street\t  interview-347303    interview-896668\r\n",
      "Richmond_Street\t\t  interview-351963    interview-9004767\r\n",
      "River_Street\t\t  interview-353218    interview-901603\r\n",
      "Rivermoor_Street\t  interview-353467    interview-901645\r\n",
      "Rockmere_Street\t\t  interview-354262    interview-90394637\r\n",
      "Rosaria_Street\t\t  interview-3588302   interview-904020\r\n",
      "Rowe_Street\t\t  interview-3609204   interview-907126\r\n",
      "Rozella_Street\t\t  interview-36398447  interview-9074626\r\n",
      "Saco_Street\t\t  interview-364735    interview-911451\r\n",
      "Saint_Saveur_Court\t  interview-36527398  interview-91673757\r\n",
      "Salutation_Street\t  interview-376115    interview-917210\r\n",
      "Sammett_Avenue\t\t  interview-37747405  interview-9185205\r\n",
      "Saunders_Street\t\t  interview-3804339   interview-920304\r\n",
      "Scotia_Street\t\t  interview-3824641   interview-92391023\r\n",
      "Searle_Road\t\t  interview-38299069  interview-92670500\r\n",
      "Selwyn_Street\t\t  interview-3871205   interview-927642\r\n",
      "Senders_Court\t\t  interview-3871242   interview-9332386\r\n",
      "Sheldon_Street\t\t  interview-38899905  interview-9346061\r\n",
      "Silver_Street\t\t  interview-3917097   interview-93473333\r\n",
      "Sparrow_Street\t\t  interview-391811    interview-93696502\r\n",
      "Spaulding_Street\t  interview-39481114  interview-938991\r\n",
      "Standard_Street\t\t  interview-39825862  interview-9408565\r\n",
      "Staniford_Street\t  interview-40534453  interview-94126412\r\n",
      "Story_Street\t\t  interview-40610944  interview-9437737\r\n",
      "Supple_Road\t\t  interview-409731    interview-944493\r\n",
      "Taunton_Avenue\t\t  interview-41553314  interview-9446528\r\n",
      "Tennyson_Street\t\t  interview-416243    interview-9501580\r\n",
      "Terrace_Place\t\t  interview-41814745  interview-95095182\r\n",
      "Theodore_Street\t\t  interview-4204949   interview-95601730\r\n",
      "Tolman_Street\t\t  interview-42161907  interview-9618669\r\n",
      "Topeka_Street\t\t  interview-4223536   interview-9620713\r\n",
      "Trapelo_Street\t\t  interview-4225866   interview-9651888\r\n",
      "Trident_Street\t\t  interview-42396365  interview-9666149\r\n",
      "Trinity_Place\t\t  interview-4262657   interview-97043057\r\n",
      "Unity_Court\t\t  interview-42934869  interview-9709892\r\n",
      "Vassal_Lane\t\t  interview-4299898   interview-9711852\r\n",
      "Victory_Road\t\t  interview-4335306   interview-9712946\r\n",
      "Vinton_Street\t\t  interview-4366523   interview-9728756\r\n",
      "Vista_Street\t\t  interview-44533008  interview-97393699\r\n",
      "Waldeck_Street\t\t  interview-4463090   interview-97409610\r\n",
      "Wallingford_Road\t  interview-448086    interview-980963\r\n",
      "Warren_Avenue\t\t  interview-45615686  interview-982013\r\n",
      "Wentworth_Street\t  interview-457117    interview-9824821\r\n",
      "Wesley_Place\t\t  interview-457451    interview-98912259\r\n",
      "Wheatland_Avenue\t  interview-466195    interview-9901455\r\n",
      "Wiget_Street\t\t  interview-4673074   interview-9912172\r\n",
      "Wiggin_Street\t\t  interview-46773428  interview-992072\r\n",
      "Wilcox_Road\t\t  interview-47246024  interview-99643550\r\n",
      "Wilkinson_Park\t\t  interview-4735823   interview-9969223\r\n",
      "Willet_Street\t\t  interview-4765278   interview-999372\r\n",
      "Williston_Road\t\t  interview-476744    lab_computer.ipynb\r\n",
      "Willowdean_Avenue\t  interview-478217    software-dev-for-ml-101\r\n",
      "Wilmington_Avenue\t  interview-48088300\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to search for filenames in Linux dir matching string:\n",
    "```linux\n",
    "find . -maxdepth 1 -name \"*string*\" -print\n",
    "```\n",
    "or alternatively:\n",
    "```\n",
    "!ls | grep 'string'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hart_Place\r\n"
     ]
    }
   ],
   "source": [
    "!ls | grep 'Hart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downhills exaggerates outgo rebating rhyming militarizes \r\n",
      "rakishly lithographs inserts \r\n",
      "humanism revolt hoggish tuner hoofs frizziest \r\n",
      "strippers toady framers bakes mandate \r\n",
      "pear graver drubbing reinvest abnormality teetotal lend \r\n",
      "stipulate attiring sprint tunnelings sera merrily proofreaders \r\n",
      "enthroning tuber onrushes pat a rows degrades parleyed \r\n",
      "probationer irresponsibility spongier smirking diminished \r\n",
      "stipulations dandle repetitious enervate \r\n",
      "reunion insert unhand heartbeat busheling teakettles dismayed \r\n",
      "plug hates easiest jabbered destining agglutinates \r\n",
      "ambitious swaggering ties annoyed battleground \r\n",
      "intuitively humbugs ribaldry suspend \r\n",
      "bedspreads expansionists restfullest \r\n",
      "dilated outstrips teapot fluoridates shops mammoth \r\n",
      "baron zoos teeter falloffs fishes invalidating \r\n",
      "patrols knelling rosining lightness unburden \r\n",
      "surfeited absurdity plexus rebuses leaking standing \r\n",
      "sizable punter bagginess dissenters \r\n",
      "postmarking spades overdraft pointillism \r\n",
      "stymie treadles extortionists dietitians agonies spitefuller \r\n",
      "multiplied submits despotism adventurous ballsier \r\n",
      "parody mania leftism tranquilize sexpots stupor parleying \r\n",
      "triumphing whiskered razing danger preordains \r\n",
      "faker preparatory gabbing mavens vamped \r\n",
      "idles tents oratorios butternuts upholsterers sassafras \r\n",
      "damnedest apologists inveigh \r\n",
      "propriety tossed parallelling town kinkier quarantine \r\n",
      "oxymoron nonmembers taproots \r\n",
      "snakebite dramatizing dawdling enlightening \r\n",
      "swastikas fragrant voluble stowing \r\n",
      "grammarian shah ambushed alines varmints doze detains \r\n",
      "wondrous uphold tests appetites wilier beautifiers \r\n",
      "drifts dosages leprosy rashers badmouth jugglers \r\n",
      "snot appals primes whet annular tonality \r\n",
      "shivery kingdom supplements envying \r\n",
      "the slayer gesture waged twenties ratifying baling mortals \r\n",
      "doggie probationary demurer fatness \r\n",
      "stupendous prolonged smog naps \r\n",
      "SEE INTERVIEW #47246024\r\n",
      "jails attune subversives forewarn adept \r\n",
      "digests militaries greased ghostwriter mating permanently \r\n",
      "hie hags twit handing bailouts bird gaggle spays germinates \r\n",
      "unsolved dilly unholiest draftier fonts \r\n",
      "milieus bleak tailspins bookshelves \r\n",
      "sheikhdom denigrated sloppier gladder \r\n",
      "rooster redesigned toady outranking obviated petards \r\n",
      "unlearned roominess and sharing twiggy \r\n",
      "snooty masques polygamist unrewarding flogged \r\n",
      "metropolitan orange buzzer guiltless \r\n",
      "shone polynomial ghostliest \r\n",
      "spar reputing insinuates jambs hoarse hep \r\n",
      "establishing throatily roomer baroque shepherd \r\n",
      "familial laser sequins overtax mulishly unsatisfying \r\n",
      "eyrie pinkest infielders flukier blaster \r\n",
      "waterway muddies unharmed fulling \r\n",
      "brogan bazookas flashers prefixing nontransferable breaker \r\n",
      "sharpshooter writhe zigzagging misgiving renal disinterment \r\n",
      "poodle trite reenlists vixenish straggler bridle tentatively \r\n",
      "nestles surname utilize skivvies suntan disadvantaged \r\n",
      "murkier orbital plebeians dehumidifiers mug homemakers \r\n",
      "paint abstinent groggier unidentified \r\n",
      "promenaded pray immodest sins pekoe bruins \r\n",
      "sheers smuggest immensities bowman law sanitariums \r\n",
      "wintered offends blogging slammers allot \r\n",
      "fluoridation droplets swab neutralizer \r\n",
      "undermost umped operated molybdenum \r\n",
      "beholder pullout pharynges dills mastered \r\n",
      "bunts mannish spurning finishing denims \r\n",
      "rumpled boastful promos demolish assassination \r\n",
      "prayer asexual doings annexes \r\n",
      "gridiron stripped exhilarating \r\n",
      "glide spellbind breadths speakers \r\n",
      "sourer radials partitions spunkiest \r\n",
      "sorrels snowdrifts healthful multiplies \r\n",
      "tempt liveliest quahaugs variably \r\n",
      "neuron ingests fathomless afghans evaluated invalids \r\n",
      "affixing joyriding pollinated headphones \r\n",
      "waywardly monitor medallions blastoffs \r\n",
      "arise granules togae liquidizes \r\n",
      "best exhibiting doming suggestive hairdressers \r\n",
      "spoilsports belong postdated wearies setups promoter \r\n",
      "fuddling sanserif ostentatiously trendies untoward \r\n",
      "bafflement inaugural dispense tinting ombudsmen \r\n",
      "totalitarians assassination prophet \r\n",
      "spandex solidness peasantry partitions \r\n",
      "purloined prevail harvests doorbell \r\n",
      "windiness oppressing repossessing notoriously \r\n",
      "wage inhuman glamorized flash junking \r\n",
      "unlawfully brainwash bawl warthogs hyper finniest \r\n",
      "piranhas drone foreseen tropospheres \r\n",
      "patients authoritarians skunks snoopiest readiest \r\n",
      "flexing unforgiving prudently sleepy merrymaking \r\n",
      "separatist spareness stake towhead sibilant \r\n",
      "poseurs snafus bilingual flushed illuminating watering \r\n",
      "lodging niter kapok etiquette handedness gravitates \r\n",
      "verging ovaries dropped edibles impresses snider \r\n",
      "metabolism stats amphitheatre rampages \r\n",
      "disbars swag heterogeneity affable presumptuous \r\n",
      "reruns slob wantonness poi averts layering orphaning \r\n",
      "plumpness heppest platooned gawkiness withdrew hostlers \r\n",
      "brine juggled brooding networking parenthesized \r\n",
      "wayside herb reap whatever triumphal phrased faltering \r\n",
      "freezing upholster tautness drollery suggesting \r\n",
      "indenture runway unintended sidestepped \r\n",
      "speedway bootstraps motivators pitying \r\n",
      "tattle dilated grunge redevelopments \r\n",
      "humdrum unbelief haggard dragonfly \r\n",
      "upholstery werewolf honorarium defiantly bomber tangential \r\n",
      "expires bidder subdivide showing pithier skateboarded \r\n",
      "safeness editors imputing idol bridges \r\n",
      "wonted figurative networked newsboys traumatizing \r\n",
      "revealing imitator handball healthfully pitiably \r\n",
      "tempered bevelling menstruation \r\n",
      "peaked notified deserting gash obligatory heeds \r\n",
      "piston hermitage smell disparate having behinds bombard \r\n",
      "powders disuse slobbers stipulate quids \r\n",
      "serialization shadings emery monotone ethnologists \r\n",
      "played housewives eventuated minivans trailblazers deportment \r\n",
      "hashed pas basket sufferer invariably \r\n",
      "poop aftershave honey penlights afforesting \r\n",
      "overlong purged outshone nest mixer solitaire stoking \r\n",
      "battleships overshadows robust statutes maunder drawn \r\n",
      "sloe bleeders nursed smelts snaky aquaria \r\n",
      "degeneration drizzle bananas \r\n",
      "extirpates tepid integrates signalizes asides savories \r\n",
      "unstablest moralizing heavenly solar notion sprains \r\n",
      "onward butterfingers panned boaters \r\n",
      "frayed avenges enfeebles heartiest piloting \r\n",
      "foretasting fishy hereby stingy panning deputy walleyes \r\n",
      "suppers blastoffs debasing threat astride \r\n",
      "primeval sleighing vast unwilling \r\n",
      "inhabit flightless shin oversupply parallelling slowpokes \r\n",
      "ventures stigmas videotape inability apartheid \r\n",
      "underbellies worshipers temptresses themselves revolutions \r\n",
      "sleepwalks agronomy ravenous \r\n",
      "smolders ruptured redistributed exhibit popped \r\n",
      "spooks unrolled elusiveness gearwheels \r\n",
      "downsized emigrations broaden barges \r\n",
      "implausibly hooliganism tilled \r\n",
      "pressurization barely ragtags gerund buffoons freaks \r\n",
      "requirement doorknobs norm plunged galaxies \r\n",
      "daze itinerary bedsores spider yea dilating ringside \r\n",
      "quintupling sentient yum philistine lasagnes saboteurs \r\n",
      "abstrusely tankards tulips planar heedlessness \r\n",
      "vibrato feasted grenadiers phasing garret outrunning \r\n",
      "puppeteer grout soundest leer papaw exterminate \r\n",
      "trendier peregrinations maternally putrid regimes \r\n",
      "davit spreads downswings privatizations \r\n",
      "woodwork fuzziness rulings heliports \r\n",
      "veneering minorities edgiest distillations infringement \r\n",
      "eras likewise tabulated raindrops \r\n",
      "treadmills violently detailed priesthood raggedness buyouts \r\n",
      "immigrants narrowness noway grow \r\n",
      "polyhedron synonym mimeographs gondolas brassier braking \r\n",
      "panderers snuggling readers gorilla deftness automaton \r\n",
      "snippy relevantly equations interned frighten \r\n",
      "governors worthily regeneration roisters \r\n",
      "olive markup skylark nuttiness redistribute \r\n",
      "proofread gentlemanly fluorite oozes \r\n",
      "usurer bogged internees vandalizing motives \r\n",
      "bulletproofs trailers numeral narrate disfigure \r\n",
      "hypo reserves fifties mail tipper \r\n",
      "jail intermediate muddles butt bankbook owes shorts \r\n",
      "glutted defeatism sagebrush louder overexposes \r\n",
      "rares pokiest siting delegated embarrass bugged \r\n",
      "priggish tonne imperially indexes restiveness antiquates \r\n",
      "senses youngster devising rumors middy sty midstream \r\n",
      "pursued windbreaker persuasiveness \r\n",
      "prevue pageants reneges refunding barrister molesting marrow \r\n",
      "platen maps nursery prohibition sweeten \r\n",
      "robot uttered guffaws bazookas obnoxious \r\n",
      "ninepin mastery hurled hassles peters sallied fealty goodbys \r\n",
      "brisked respired dimwits pupas writhing \r\n",
      "requirement item risky diseased adjuring \r\n",
      "bribe retaliations playgoer wholly naysayers baseball \r\n",
      "eyelids selflessness sidestroke stationed peeing \r\n",
      "pantomimed nominal gulls patrolmen \r\n",
      "reorders blindfolded agates gallbladder grabbed \r\n",
      "aged adenoids stewardess mutilating \r\n",
      "raindrops tipsily beautify wimpiest \r\n",
      "sandstorms absolutism bittersweet bluffed halogen enshrouding \r\n",
      "manifesting booziest vermilion quizzes luxuriantly deflates \r\n",
      "pleating finalizing explain endearment disengaged \r\n",
      "anxieties smokes domineer depressed \r\n",
      "sterilizes parasites initialization punish apropos \r\n",
      "strained beget devoted shawl tabloid buzzing plushier nibs \r\n",
      "deservedly tingeing rill grinning pervert patina airworthy \r\n",
      "wrongly jokers pinwheels sheik king smut thumbnails \r\n",
      "beginners downsized paperwork apses \r\n",
      "harm abloom entreaties submersion partaker \r\n",
      "snowmobiling engorging helpmeets fauna wardrobe bridles \r\n",
      "tutus portliest anagram addressees pansies pinioning foreshadows \r\n",
      "value progressed eggshells manliness matronly weaponless \r\n",
      "narrowing refutation antiperspirant tern \r\n",
      "deeds markups preterits eyes dissipation \r\n",
      "propounding ballpoint snowfall gropes \r\n",
      "iota filmy runes astonishingly await prepping \r\n",
      "gaffes tors stupors rearranged bushel last kudzu \r\n",
      "lurks unperturbed mutter tunnies die flexibility \r\n",
      "hookers usurpation remarkable barefoot \r\n",
      "stoves westerners haemorrhoids enrolls dumpiest \r\n",
      "realms stamen babe purists disunites glottides talon derivations \r\n",
      "using affords emphasized provide shire \r\n",
      "hardwood reverberations streakiest planar joyride rush \r\n",
      "prepaying felons rehash housebroke ramble \r\n",
      "filler saviors abruptest jaw buttoned abrading \r\n",
      "handguns assistants proprietaries \r\n",
      "fiendishly imitation dust egged baptistery \r\n",
      "regenerates stifling sprats propounded \r\n",
      "menfolk monsignors pi variations epaulettes \r\n",
      "phrasing peeped palpates tonsorial \r\n",
      "disdainful minimally sonar yeshivahs kebabs \r\n",
      "flypapers employment snowdrift displease swishest blurs \r\n",
      "skitter rundown reproof supernatural muezzin nerve \r\n",
      "ordained paps jostled dourly marvelously \r\n",
      "soppier longhairs neurons daybeds \r\n",
      "dyeing q braziers twosomes defoliated lavishing \r\n",
      "employers wiggled ratifying remount doodads lorded \r\n",
      "usher makeshifts underskirts parakeet bulletined immigrant \r\n",
      "kneaded spouting maneuverability vegetative \r\n",
      "defraying mourned hookers assails roan steamrollers \r\n",
      "seaway demilitarized inky roseate \r\n",
      "repeater underlines plunger gentlest monograph \r\n",
      "grimiest upturned muter meteorologist dialog \r\n",
      "nooses voluptuary jigs rewards ignited \r\n",
      "parsing drifter dinners hypersensitivities \r\n",
      "abrogation herpes papaw herself sullen reassemble evenhanded \r\n",
      "pharyngeal biplane amount turned saplings \r\n",
      "willingness hardwoods mores jogger tantalize \r\n",
      "sandbox mainly terribly absents \r\n",
      "lagers veggies feigning finders \r\n",
      "fogbound fennel lobbed tabulating \r\n",
      "imprisoning motivated fellatio \r\n",
      "expatriates mizzenmast slant swished unnerves \r\n",
      "ashamed unmasked blindly tirelessly \r\n",
      "transmigrate navigates spinsterhood rustler presentiments \r\n",
      "wastefulness retaliated jobless jury saprophytes graveled \r\n",
      "quote seventy sorters happiest \r\n",
      "waterfronts indispositions gossipping \r\n",
      "sunbathers toll averred timed partners demotion \r\n",
      "squawks pantomime dissembled seizures \r\n",
      "earning diminutives bittersweets biographers \r\n",
      "drapery quintets pretends peps gaberdine gates quartermasters \r\n",
      "disdainful qualifying deflated babels internees \r\n",
      "rower gamekeepers ensured bequeaths \r\n",
      "supportive nipping mannish wanna garners \r\n",
      "overlap antipathy vaguely staying dubiety reformulate \r\n",
      "blousing germane uninstall inexpensively shrimps \r\n",
      "lipreads supporter ashed loopy toots belong ante umpire full \r\n",
      "parallelism iterated tune bondsmen buffaloes \r\n",
      "parleying pardon gingersnaps shoots molted \r\n",
      "busied intangible valving wingers retrograde tangent \r\n",
      "mummified reeked toad illuminate steamier \r\n",
      "bountifully embeds streamlined wigwag \r\n",
      "bedevilment limits movers ferrets marts manifolds \r\n",
      "surpluses high auditorium nodules \r\n",
      "nuns guardroom folklore surtaxing whirlpool \r\n",
      "overbooked hosteler embezzling foresees quell debugger \r\n",
      "wallow gotta proofreader flexibly \r\n",
      "gnat believing peyote rudiment hays alderwomen \r\n",
      "undemanding livening maximal senates retinues \r\n",
      "pharynx above madman them slanted \r\n",
      "skimped hyphenations interleaved repainted attributively \r\n",
      "stalls benumbing squint overtakes pearlier \r\n",
      "tastefully medieval demagnetizing \r\n",
      "mislaid perplex liniment bigamists outburst overeating \r\n",
      "nannies mongering slavered pundit tunneled tankards \r\n",
      "reimbursement naming enslavement sneered disorder \r\n",
      "addresses unholiest simulated squalling \r\n",
      "dregs parody stashes systematizes him humoring despot \r\n",
      "interplay tarts eyeballing lounged funerals \r\n",
      "by dillies awkwardly seamier wiliest speeder \r\n",
      "sunk huntress stopgap oversells respondent \r\n",
      "trialing swains have sitting post holidayed minimizing \r\n",
      "plenitude alabaster military sweetened \r\n",
      "misspend appeasements filthy dryly winking beheaded landlord \r\n",
      "sutures laundrymen unreadable leafletting \r\n",
      "somewhat seaport proves abbeys ravens \r\n",
      "soya meg truffle industrialist perpetrator \r\n",
      "sop mobsters surrenders flexibly begins trumpeting barn \r\n",
      "geode axe unhealthier diss suborn mastheads rebounds \r\n",
      "anthrax unsafer aneurism bountiful portmanteaux bust endorsing \r\n",
      "gigged bruisers deporting separates unpaved \r\n",
      "outward firefighter threading moratoriums \r\n",
      "inhabitant lanky vulnerable yawning beautifiers rustiness \r\n",
      "tabbing saviour slayings granulates outed beast \r\n",
      "misdeals monotonously shedding \r\n",
      "fifth kindliness heartbreaking sterling reinvesting \r\n",
      "flashiness earthliest tub overhang promotes \r\n",
      "angstroms belfries abuts positing majorettes goutier \r\n",
      "feelings derringers regeneration septuagenarians \r\n",
      "arboreta finisher meld abstainers robuster \r\n",
      "retires agglomerated frigates heartwarming \r\n",
      "runt airfoil looniest wallpapered embossing \r\n",
      "muttering reapers spokesman demolished spotter \r\n",
      "unjustly unmarked aggrandized sappy \r\n",
      "impairs gurgles intrusts shamefully \r\n",
      "pasting beekeeping possibility beaning toaster tomb gores \r\n",
      "sensually standings glinted beastlier \r\n"
     ]
    }
   ],
   "source": [
    "!cat Hart_Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEE INTERVIEW #47246024\r\n"
     ]
    }
   ],
   "source": [
    "!grep 'INTERVIEW' Hart_Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckingham_Place\r\n"
     ]
    }
   ],
   "source": [
    "!ls | grep 'Buckingham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one earthlings startles invitingly pall \r\n",
      "headwaiter mate impregnability \r\n",
      "unmake drainpipe utilities pointillist \r\n",
      "apropos impressively forborne finite exempt \r\n",
      "griming vised thankfully burlap hypertension \r\n",
      "landsliding landfill furlong mittens heartland \r\n",
      "bully tortoise enlargers roamed undressing \r\n",
      "yuks troubleshooting seaboards \r\n",
      "springtime deaves reinitialize puttying \r\n",
      "densities warranties penultimates dehumanizes perorations \r\n",
      "untangles stays smashing spinets breadfruits \r\n",
      "granulating toreadors finishes knob headhunter \r\n",
      "longhairs pennyweights womanizers \r\n",
      "depressive overused disturbs glandular pillowed \r\n",
      "fallibly proportioning settling jumpier \r\n",
      "vagueness lethal alienating potted \r\n",
      "immortalizing parfaits ignited malnutrition \r\n",
      "feisty senseless manly fifths tailspin supposed downstairs \r\n",
      "ringer drainers agiler pinholes reedier \r\n",
      "meekest revolvers gobblers panelists unassigned yew butterfly \r\n",
      "nuts singsonged writs arm dimness \r\n",
      "bearing shags kleptomania sprinted barkers \r\n",
      "augury bullied letups shimming filmy golds \r\n",
      "thunders forbid snowflake unstabler moralist \r\n",
      "torrential attributable poniards newsletter stealthier \r\n",
      "breastplates bundled nationalization \r\n",
      "invaliding fitfully figured keystones \r\n",
      "misdoing unplugged newspapermen grandstand \r\n",
      "shrouding pushed botanists pivots domed \r\n",
      "mealier formerly trivet avenues flukey humorously \r\n",
      "twofold sniffling misdoes intros bookmarked \r\n",
      "subordinated amateur ambition tarpons fluttery \r\n",
      "gassier soldiering irritant wavelengths wriggling \r\n",
      "tarots gruesomely pair minks enrapture \r\n",
      "stupefy dukedoms emanating manikin enshrines \r\n",
      "freaky meditated paginating suddenly farewells presentations \r\n",
      "darted sizes tartly meddlers startled \r\n",
      "strong love thumbnail aquifer samurai \r\n",
      "parleys mobilizing repertoires sanitarium punts \r\n",
      "entwining inkier exterminate remission purblind \r\n",
      "inflammations resist impedes lighthouses allegro \r\n",
      "additions pantomiming wrongdoing natives elusive opals \r\n",
      "finagles assaying raving primmest libel repairable \r\n",
      "gallbladders dismissed girders waxworks tenderly feasibly \r\n",
      "temporaries peephole whams faultfinding metabolizing \r\n",
      "floor huddles refinish flattering swamped \r\n",
      "levied whets undersign bestiary \r\n",
      "auras kite presumes toasty pair loping revelled teas abnormally \r\n",
      "reality polarized preferably nominated \r\n",
      "pawnshops bootie pealed violist haulers haled \r\n",
      "palmettoes waste spot soggier annulled \r\n",
      "steak shrews gunshots foregone kneel \r\n",
      "inlets bran maraud salts hemmed \r\n",
      "poniard mangled dither humblest demitasses proprietary \r\n",
      "augury telemetry starlit landlubbers \r\n",
      "portrayed battleships orderliness salver forming grits \r\n",
      "alleyways disowning blogged argosies \r\n",
      "supervisor adored unequalled dangle nonreturnables \r\n",
      "sixteens defaulted bandoliers turtledove \r\n",
      "gearwheels junks endlessly wear internship timidly maxim \r\n",
      "waver windbreaker sunken disguising importations \r\n",
      "shoots smoggiest lifestyles journeymen \r\n",
      "reffed raft futzing dagger surrealism \r\n",
      "unsupervised disrepute ingeniously annul divining \r\n",
      "expressing rib snorted hexagons sway sharing neurosurgery \r\n",
      "frazzled leaving aqueous stigma punning psalmists \r\n",
      "desirous devaluation pudding alight squealed \r\n",
      "marital unsubstantial peasant drying bee agglomerates \r\n",
      "hansom methought adapts muezzins relish \r\n",
      "eastward maydays gondola burger abstains predetermined gated \r\n",
      "ornithology unarmed godfather roomer penises primitives \r\n",
      "slumber ether mender edgy underhanded ablest affirmatively \r\n",
      "sex barbershops leotards bobolink hormonal sump \r\n",
      "indubitable yardages felons unquestionably versifies infinitesimals \r\n",
      "aliasing inhibiting originally synthesizes \r\n",
      "perjuries goblet pasta moves farrow urged dotes fluffs \r\n",
      "liberals indentation horns erosion harrow snowstorm emulator \r\n",
      "pretending expiation imperialist overdresses \r\n",
      "bookmakers analogues dittoed effigy thunderheads \r\n",
      "employee wafer pessimist alleyway envisages \r\n",
      "orderings bedbug employ muskmelon benumb \r\n",
      "shards ominous bawl worryings refills animators soften \r\n",
      "merrier vanadium provides medleys obstinately summoner \r\n",
      "fold swoons grievously dismantles \r\n",
      "unbroken hothead jessamine yolk loath rake \r\n",
      "bludgeoned inflationary dowdy fingers saunter \r\n",
      "great diminutive puffiness bankbooks \r\n",
      "insolvent more balms millions mommy river sullenest \r\n",
      "intertwines obeisant gallon blame tub seeking firsts \r\n",
      "lingered populates naughty galvanizes postulated \r\n",
      "frailest shin wrestle faking goddamed allergist minute lobotomies \r\n",
      "thrower industriously idolized unabashed understudied \r\n",
      "shaking refereed mullet harmfulness hasp \r\n",
      "tubeless irretrievable transits underwrites drainage \r\n",
      "renegaded bilk gearbox expertise away beating \r\n",
      "rapt preventative freedman trouts insulted peopling pales \r\n",
      "earnest blabbing prohibitionist deserts runway \r\n",
      "patents idiot marvel deadens shad sluggers essay talkative \r\n",
      "sneer lows sidled sparingly ink geranium lighthearted \r\n",
      "boudoirs merry juniper while liquifies phobias goad peanuts \r\n",
      "hesitating inverse nighttime \r\n",
      "loosely impish threaded formlessness signet \r\n",
      "knotted pursues sightseer history software \r\n",
      "admonishment underskirt speedway deviate fags \r\n",
      "sandblasting plodding faggots grungy eventuality \r\n",
      "workmen extemporaneous espouses straitening laxatives \r\n",
      "harangue summing binderies membership \r\n",
      "huskies sidesaddles restoration tawdriest gazillions \r\n",
      "liven delimit jibe profited butterfly faithful \r\n",
      "iodize wheezier ministration hotbed rehabbing nuzzle \r\n",
      "esquire swallows smelly grounder shimmering sharpers hexed \r\n",
      "legionnaires workdays rinds personality \r\n",
      "governor floury earrings overusing \r\n",
      "winsomer falsity paradigm rows wiry alas worship \r\n",
      "spears goaded took despaired betrothal mink doled penetrative \r\n",
      "ingot tasting uprights barks sottish troubleshooter \r\n",
      "persevere ping deafening kisser innovators \r\n",
      "gaudier noiselessly glamorously quasar beeping yesterdays \r\n",
      "spinal klutzes yeastier initializing \r\n",
      "foetuses brews blunderbuss emitting \r\n",
      "hillier streak opines retirement straightedges nontransferable \r\n",
      "mending envisions gizmos drags finnier wildest \r\n",
      "adder malingerers euphony antiphonal departs spitfires \r\n",
      "hypes dye illuminations disablement \r\n",
      "sender waned bottomless saintlier lades plethora \r\n",
      "shenanigan larynges penguin heatstroke \r\n",
      "steins exiles underestimating guts billet flabbergasting \r\n",
      "sieges flawlessly serialized kippers \r\n",
      "enfeeble weathered bedridden liquidations \r\n",
      "flints winners unfairest damaged \r\n",
      "partnered pilaws hogs pluming avenge \r\n",
      "tanker forwent sag pheromone snowdrifts syphons \r\n",
      "mimes mountaineers exalted disfigure shelves nimble \r\n",
      "flusters derivable potato establishment \r\n",
      "regimen betterment sarape misused balladeer pilling \r\n",
      "besiegers whitens dinky stupid \r\n",
      "kettle borax partnering preppiest \r\n",
      "fisherman dappling mustier fobs figuratively orthopaedists \r\n",
      "jumpiest tundra hybridizing \r\n",
      "inexhaustibly thrower metaphors intones slanted \r\n",
      "insinuated garrison prettifying attests \r\n",
      "stemming shrewdness prigs workweek feebleness abiding distrust \r\n",
      "sternest rowel debater peas adversely enshrouds \r\n",
      "premiss nowhere billion gatherings fetid traded \r\n",
      "misappropriation donations printers \r\n",
      "dethronement holler spurned divisors \r\n",
      "reverts plaza obsequiously permutation \r\n",
      "grandpas tyrants metastases personae hauler eyelash \r\n",
      "outrage shelters soured surlier metronome moonlighted \r\n",
      "mailmen maneuverable mattered seeding dehumanizes \r\n",
      "spiniest dents weightless kookaburras misprinted disorders \r\n",
      "intermediary selling jersey polymaths empathizing \r\n",
      "doubling peaked owlish byline levitates suspense relabels \r\n",
      "autumns wiping dhotis damming twentieth proportion \r\n",
      "potentate bribes rifer moderate verbena desire framing \r\n",
      "twaddle polite pretext mazourka situated \r\n",
      "bilking darling tempera stresses earmarking \r\n",
      "breezes fretted exhumed fertile \r\n",
      "vineyards gingerbread ridgepoles helixes \r\n",
      "deal heppest malignantly tallest ebony \r\n",
      "flexible espressos prioresses linoleum deposed tourism \r\n",
      "sophomore filtering prostitute wasps \r\n",
      "parboiling sine rain unprofessional \r\n",
      "surrey mudslide brilliants planing derail wilder \r\n",
      "environmentally professor frenzies trolled \r\n",
      "watermelons garrisoning imprisonments typified \r\n",
      "busied besetting founders flurried state trill \r\n",
      "hyphened flashiest advertise byline patrons fighter goldsmith \r\n",
      "sightread teazles footfall washbasin molesting \r\n",
      "grainiest lobbing shrouded largest jump geometries \r\n",
      "eluded sophism stages pertly sewerage biplanes maturer travelogs \r\n",
      "fords plaque droned spars misleading sunburn \r\n",
      "serenely shits dialyses roistering registrars pointillists \r\n",
      "bales napes warts mammary referrals fisherman training \r\n",
      "panting battalion greedier allergens orotund \r\n",
      "headers badge bullhorn agglutinating bearable \r\n",
      "analogs departing anguished nylons \r\n",
      "whitewall napkins overheats embittering grimmer beamed \r\n",
      "SEE INTERVIEW #699607\r\n",
      "festoon dispenser kidder odysseys spoons buttonholing \r\n",
      "arguably manse lessened logging pasts \r\n",
      "holler burg spotlight quadrupeds glitzy froths harmony \r\n",
      "bashfully blotter bushy prettifies \r\n",
      "invent baseness heave flunking temerity \r\n",
      "noose harms grebe attributively ruling \r\n",
      "swat reissues founts envy gingko nuzzled albums \r\n",
      "pippins deathblows hounds voodoo buffed \r\n",
      "politer volunteers overtaxed quoted bakery \r\n",
      "mute jasmine imbues pejorative implementing \r\n",
      "flooded immortal rebuking enthralling \r\n",
      "value bebops brasses mono toileting \r\n",
      "kindle ghostwriter moodier original abridging \r\n",
      "palliation fumbler elapsing marauders honorariums \r\n",
      "invaluable sauntering spuriously phalanx marry plights \r\n",
      "blameworthy bandages gentlemen playboys rosebuds \r\n",
      "norms masterminding milkier demoralization leaven disallowing \r\n",
      "supersedes ambivalent dearness organizational \r\n",
      "sinks resemble noisiest peppy feedbag \r\n",
      "wrapper grotesquely importunity retrograded struggle deviling \r\n",
      "stymie gangrene rapine nevermore digested whirling \r\n",
      "freshets adverbial addles amalgamating forest informants \r\n",
      "motorboats sadism drouths swaggering foursquare ermine \r\n",
      "paperweight pub pitied yearning reeving \r\n",
      "polish penes tortoises immaturity trellis timetabling \r\n",
      "horsehide putting raved meditate inertia thoughtfulness \r\n",
      "savaged saviours disburses tonsil \r\n",
      "bind ruffle mausolea enjoined whiskeys dozen tomato shortsightedness \r\n",
      "hampering artworks jehad manifested \r\n",
      "buffing peeking dart roils omission trumpery \r\n",
      "kippered fruited snuffled unmitigated mansards spies \r\n",
      "dumpster relate meadow hands boardwalk kites \r\n",
      "takeaways goatskins detail ogre spurs \r\n",
      "ruminant piggish fiesta remodel \r\n",
      "nonstop suggester mess wheedles denied enjoyed \r\n",
      "dishtowel ponytail spotlessly barman \r\n",
      "blindest arraignments showroom provably \r\n",
      "manful mandrills gangplanks glandular \r\n",
      "manganese futurities suffragette \r\n",
      "metamorphosis bounders leeriest hyperventilated waterfall \r\n",
      "pitiable tarragons interring drawl \r\n",
      "emotional reexamined revelries slipperier forwarding potentates \r\n",
      "outliving suspending stippled hesitates rosary aliased \r\n",
      "expatriated delivered quintuplets \r\n",
      "razors assortment sparser yews \r\n",
      "relabeled snoop resales hornier berets squeezers \r\n",
      "jinn guttered untreated vegetarian waterspouts \r\n",
      "broadest ashes piquant oftenest nether sheering surrounded \r\n",
      "stewarding tanner foghorn repasts orders \r\n",
      "weer badmouths rape lug utter donkey \r\n",
      "gybing veered rely metering harmfulness bombshells \r\n",
      "peeps regressing pasture skywriter \r\n",
      "proportional trio gelled shivery inferiority \r\n",
      "leaflets pilgrims guileful inflates playfully tarpaulins \r\n",
      "ugh pompons simulate numbers martyr introverts aerie urgent \r\n",
      "stage powering ermine enquire merrier stalker \r\n",
      "undoubted urban illumined flotillas downtrodden gelatine \r\n",
      "abbreviated skiers grumpy attesting gals roil \r\n",
      "synthesizer tared dungarees headlights pluralities module \r\n",
      "preponderating skylarks lawyer swordsmen stupefying motliest \r\n",
      "upstream garnered sprightlier explodes intervenes drawings \r\n",
      "antitrust brandies railroads toenails inveighs \r\n",
      "laded gives quibbler nonplused parliament meadowlarks \r\n",
      "dandled snafu angiosperm hamburgers adverser snowfall \r\n",
      "dogmatism jousts bullshits esquires observable \r\n",
      "request tended reported rhapsody grandstands \r\n",
      "fantasied wonderlands mutually tiptoeing misdeeds \r\n",
      "endue liquidation tinging redistributing \r\n",
      "studied predisposes gauges agenda airfoils \r\n",
      "himself reformation twenty types rotundness file blearier \r\n",
      "jokers sharpest hookahs zinged shutting bestowing \r\n",
      "linkage brothel faggots lisp runway relation harboring \r\n",
      "plurality gazette lively impostures youngest \r\n",
      "refunds supported headroom trekked outtakes interrogation \r\n",
      "fashionable inhibits trumpeting pizazz \r\n",
      "semimonthly shuddering signer roughshod \r\n",
      "epitaphs stales dipper debarment wader \r\n",
      "marinades flashily dispossessed \r\n",
      "segmenting examine appoints granddads molt four monorails \r\n",
      "gibe aspirating vulgarities fetishes lenses \r\n",
      "extroversion vibrantly lubbers kiss haw \r\n",
      "exult enslavement mutuality maturities ousted \r\n",
      "arson hogwash tush bouquets donor disrupting landsliding \r\n",
      "furtively rhapsodies inky laddered wringing rosebud \r\n",
      "limpid hobos shimmering relying \r\n",
      "likewise inquests kid pinnate insignias journeys \r\n",
      "greyed hewing surtaxing hydrology transfigures horded \r\n",
      "suborned whirlpools hobnails aligning agent logotype \r\n",
      "hatefully homeopathy riskiest ilk flagpoles burglarizes \r\n",
      "sundown delphinium normalized \r\n",
      "amnesties trellising rolled misrepresentations \r\n",
      "avast hankerings abominates sloughing \r\n",
      "heartrending bullish removers fraternity dignified \r\n",
      "mettlesome prowlers beautiful manager lollygagged \r\n",
      "suburbs donut gunrunner supports denotes seraphim polyunsaturated \r\n",
      "origination longevity snowballing foretasted sunlight obliteration \r\n",
      "slipping tranquillizers perfumeries \r\n",
      "disaster titled mitigated inseminates populating \r\n",
      "thirsty burros disproved damned validate \r\n",
      "hookworms what magpies weeknight lolls mixing \r\n",
      "dragooning stipple gangway leafletted \r\n",
      "possibles downfall inspire lobbies \r\n",
      "substituted nightshirts delimiters hereof ulna benumb \r\n",
      "hilltops beeped apologizes ties skewed fate \r\n",
      "needles weekending provisioning emended transfusion \r\n",
      "rifer pipers dogfish strafed stereotyped \r\n",
      "junked wooer vaulters sodium dynamism \r\n",
      "looting goodbys tweets uniquer bibulous lodestone \r\n",
      "hospital gnarl fathomless barrings implies manners tunneled \r\n",
      "debasement loathsomeness tabued wraith \r\n",
      "shout shed telepathy reparations filthy brandish \r\n",
      "instrumentalists fatally tattooists anonymity biography \r\n",
      "prostrate refill dreads belaying \r\n",
      "riposted adaptive bawling applauding mushes toileted sleazy \r\n",
      "juggernauts leis downstream dismounts propriety hit \r\n",
      "rite boogies argument shampooed semitone dork \r\n",
      "voluntary ravages draftier purports \r\n",
      "disproves uphold randomized flexes reassures snoopiest \r\n",
      "quadruple hampers assureds blindsides blab pay woodmen motivations \r\n",
      "daydreams jalopy remarking engraves \r\n",
      "quasars delineate blanket unstudied hoax skiff \r\n"
     ]
    }
   ],
   "source": [
    "!cat Buckingham_Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEE INTERVIEW #699607\r\n"
     ]
    }
   ],
   "source": [
    "!grep 'INTERVIEW' Buckingham_Place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the interviews\n",
    "\n",
    "Now that you have the interviews you want to check out, let's see what they say!\n",
    "\n",
    "We'll use the `glob`, and `os` standard library to collect all the data together into a usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/interviews/interview-000296', './data/interviews/interview-00448418', './data/interviews/interview-00502304', './data/interviews/interview-005702', './data/interviews/interview-00617019', './data/interviews/interview-00805135', './data/interviews/interview-016463', './data/interviews/interview-020337', './data/interviews/interview-022751', './data/interviews/interview-0234126', './data/interviews/interview-02422821', './data/interviews/interview-0251720', './data/interviews/interview-03098229', './data/interviews/interview-0315125', './data/interviews/interview-03316077', './data/interviews/interview-034070', './data/interviews/interview-0349327', './data/interviews/interview-04393507', './data/interviews/interview-044492', './data/interviews/interview-0462097', './data/interviews/interview-049721', './data/interviews/interview-05297663', './data/interviews/interview-06032377', './data/interviews/interview-0613334', './data/interviews/interview-066291', './data/interviews/interview-071537', './data/interviews/interview-0732631', './data/interviews/interview-07497003', './data/interviews/interview-0768255', './data/interviews/interview-092423', './data/interviews/interview-0953437', './data/interviews/interview-096267', './data/interviews/interview-102490', './data/interviews/interview-109118', './data/interviews/interview-1108561', './data/interviews/interview-114661', './data/interviews/interview-11495001', './data/interviews/interview-116803', './data/interviews/interview-11705111', './data/interviews/interview-11783660', './data/interviews/interview-11817172', './data/interviews/interview-1186827', './data/interviews/interview-1205060', './data/interviews/interview-1250176', './data/interviews/interview-125204', './data/interviews/interview-125271', './data/interviews/interview-1269181', './data/interviews/interview-1310392', './data/interviews/interview-13768464', './data/interviews/interview-13889608', './data/interviews/interview-13920860', './data/interviews/interview-1395414', './data/interviews/interview-141030', './data/interviews/interview-14153840', './data/interviews/interview-144873', './data/interviews/interview-14590717', './data/interviews/interview-147283', './data/interviews/interview-15187437', './data/interviews/interview-15354942', './data/interviews/interview-1536668', './data/interviews/interview-155049', './data/interviews/interview-1578206', './data/interviews/interview-159848', './data/interviews/interview-16098538', './data/interviews/interview-1642421', './data/interviews/interview-1643440', './data/interviews/interview-16889008', './data/interviews/interview-17248453', './data/interviews/interview-17343208', './data/interviews/interview-174898', './data/interviews/interview-1767435', './data/interviews/interview-17827186', './data/interviews/interview-179719', './data/interviews/interview-1811770', './data/interviews/interview-18193261', './data/interviews/interview-1823688', './data/interviews/interview-18270219', './data/interviews/interview-18441251', './data/interviews/interview-1850922', './data/interviews/interview-1857368', './data/interviews/interview-1906958', './data/interviews/interview-191206', './data/interviews/interview-19300543', './data/interviews/interview-1933118', './data/interviews/interview-19577850', './data/interviews/interview-2058907', './data/interviews/interview-210355', './data/interviews/interview-218131', './data/interviews/interview-221039', './data/interviews/interview-223913', './data/interviews/interview-2277882', './data/interviews/interview-229443', './data/interviews/interview-23167806', './data/interviews/interview-2326746', './data/interviews/interview-23371263', './data/interviews/interview-233800', './data/interviews/interview-2415821', './data/interviews/interview-243703', './data/interviews/interview-2481877', './data/interviews/interview-250112', './data/interviews/interview-253705', './data/interviews/interview-255531', './data/interviews/interview-25582311', './data/interviews/interview-25834905', './data/interviews/interview-259909', './data/interviews/interview-2601508', './data/interviews/interview-26373485', './data/interviews/interview-2642139', './data/interviews/interview-27042476', './data/interviews/interview-27504937', './data/interviews/interview-275706', './data/interviews/interview-279087', './data/interviews/interview-280877', './data/interviews/interview-2834518', './data/interviews/interview-284560', './data/interviews/interview-2846076', './data/interviews/interview-289524', './data/interviews/interview-290346', './data/interviews/interview-291440', './data/interviews/interview-2922290', './data/interviews/interview-29316965', './data/interviews/interview-2939888', './data/interviews/interview-296128', './data/interviews/interview-29680692', './data/interviews/interview-29741223', './data/interviews/interview-2976680', './data/interviews/interview-29838622', './data/interviews/interview-2995681', './data/interviews/interview-301018', './data/interviews/interview-30259493', './data/interviews/interview-3049045', './data/interviews/interview-305694', './data/interviews/interview-305949', './data/interviews/interview-306616', './data/interviews/interview-3074127', './data/interviews/interview-3099757', './data/interviews/interview-312546', './data/interviews/interview-3128999', './data/interviews/interview-3140662', './data/interviews/interview-31635890', './data/interviews/interview-3201508', './data/interviews/interview-322305', './data/interviews/interview-32365018', './data/interviews/interview-324389', './data/interviews/interview-325611', './data/interviews/interview-32639981', './data/interviews/interview-32712166', './data/interviews/interview-331178', './data/interviews/interview-332596', './data/interviews/interview-33399976', './data/interviews/interview-340396', './data/interviews/interview-34041151', './data/interviews/interview-342393', './data/interviews/interview-34359897', './data/interviews/interview-344331', './data/interviews/interview-34690644', './data/interviews/interview-347303', './data/interviews/interview-351963', './data/interviews/interview-353218', './data/interviews/interview-353467', './data/interviews/interview-354262', './data/interviews/interview-3588302', './data/interviews/interview-3609204', './data/interviews/interview-36398447', './data/interviews/interview-364735', './data/interviews/interview-36527398', './data/interviews/interview-376115', './data/interviews/interview-37747405', './data/interviews/interview-3804339', './data/interviews/interview-3824641', './data/interviews/interview-38299069', './data/interviews/interview-3871205', './data/interviews/interview-3871242', './data/interviews/interview-38899905', './data/interviews/interview-3917097', './data/interviews/interview-391811', './data/interviews/interview-39481114', './data/interviews/interview-39825862', './data/interviews/interview-40534453', './data/interviews/interview-40610944', './data/interviews/interview-409731', './data/interviews/interview-41553314', './data/interviews/interview-416243', './data/interviews/interview-41814745', './data/interviews/interview-4204949', './data/interviews/interview-42161907', './data/interviews/interview-4223536', './data/interviews/interview-4225866', './data/interviews/interview-42396365', './data/interviews/interview-4262657', './data/interviews/interview-42934869', './data/interviews/interview-4299898', './data/interviews/interview-4335306', './data/interviews/interview-4366523', './data/interviews/interview-44533008', './data/interviews/interview-4463090', './data/interviews/interview-448086', './data/interviews/interview-45615686', './data/interviews/interview-457117', './data/interviews/interview-457451', './data/interviews/interview-466195', './data/interviews/interview-4673074', './data/interviews/interview-46773428', './data/interviews/interview-47246024', './data/interviews/interview-4735823', './data/interviews/interview-4765278', './data/interviews/interview-476744', './data/interviews/interview-478217', './data/interviews/interview-48088300', './data/interviews/interview-48148020', './data/interviews/interview-483817', './data/interviews/interview-485229', './data/interviews/interview-4950099', './data/interviews/interview-4961376', './data/interviews/interview-496772', './data/interviews/interview-498331', './data/interviews/interview-499096', './data/interviews/interview-50168425', './data/interviews/interview-50291987', './data/interviews/interview-504687', './data/interviews/interview-509105', './data/interviews/interview-5143029', './data/interviews/interview-514793', './data/interviews/interview-52280505', './data/interviews/interview-528044', './data/interviews/interview-529706', './data/interviews/interview-53318557', './data/interviews/interview-535181', './data/interviews/interview-5372865', './data/interviews/interview-538900', './data/interviews/interview-54026669', './data/interviews/interview-541518', './data/interviews/interview-5455315', './data/interviews/interview-54619323', './data/interviews/interview-54851634', './data/interviews/interview-549055', './data/interviews/interview-55382746', './data/interviews/interview-55410365', './data/interviews/interview-55435298', './data/interviews/interview-55477243', './data/interviews/interview-555536', './data/interviews/interview-5581158', './data/interviews/interview-55841398', './data/interviews/interview-55984022', './data/interviews/interview-565396', './data/interviews/interview-566707', './data/interviews/interview-56784802', './data/interviews/interview-56892213', './data/interviews/interview-57236791', './data/interviews/interview-5739404', './data/interviews/interview-5766907', './data/interviews/interview-5774468', './data/interviews/interview-5782759', './data/interviews/interview-579105', './data/interviews/interview-5835471', './data/interviews/interview-586668', './data/interviews/interview-58910793', './data/interviews/interview-5905106', './data/interviews/interview-591273', './data/interviews/interview-5993978', './data/interviews/interview-60081985', './data/interviews/interview-604403', './data/interviews/interview-608607', './data/interviews/interview-6093093', './data/interviews/interview-618764', './data/interviews/interview-6203192', './data/interviews/interview-628618', './data/interviews/interview-63308519', './data/interviews/interview-637657', './data/interviews/interview-637928', './data/interviews/interview-638121', './data/interviews/interview-6417794', './data/interviews/interview-645385', './data/interviews/interview-6553472', './data/interviews/interview-65792229', './data/interviews/interview-659803', './data/interviews/interview-66101490', './data/interviews/interview-66282920', './data/interviews/interview-6643191', './data/interviews/interview-67279454', './data/interviews/interview-673985', './data/interviews/interview-676473', './data/interviews/interview-67790846', './data/interviews/interview-680549', './data/interviews/interview-6808205', './data/interviews/interview-68195573', './data/interviews/interview-68488577', './data/interviews/interview-68764140', './data/interviews/interview-6884359', './data/interviews/interview-6894000', './data/interviews/interview-69170457', './data/interviews/interview-6933068', './data/interviews/interview-699607', './data/interviews/interview-70067280', './data/interviews/interview-70199425', './data/interviews/interview-703831', './data/interviews/interview-704443', './data/interviews/interview-70458099', './data/interviews/interview-7046684', './data/interviews/interview-7066082', './data/interviews/interview-706620', './data/interviews/interview-707438', './data/interviews/interview-708943', './data/interviews/interview-7103823', './data/interviews/interview-71186817', './data/interviews/interview-71226767', './data/interviews/interview-71298441', './data/interviews/interview-7180973', './data/interviews/interview-71993338', './data/interviews/interview-720268', './data/interviews/interview-7254073', './data/interviews/interview-728181', './data/interviews/interview-730123', './data/interviews/interview-73035802', './data/interviews/interview-7305678', './data/interviews/interview-73585672', './data/interviews/interview-737609', './data/interviews/interview-7422077', './data/interviews/interview-74225310', './data/interviews/interview-7469675', './data/interviews/interview-7541406', './data/interviews/interview-75434722', './data/interviews/interview-755037', './data/interviews/interview-75633580', './data/interviews/interview-7580872', './data/interviews/interview-77014856', './data/interviews/interview-770439', './data/interviews/interview-77135281', './data/interviews/interview-7791374', './data/interviews/interview-780255', './data/interviews/interview-7863761', './data/interviews/interview-789564', './data/interviews/interview-791289', './data/interviews/interview-79360358', './data/interviews/interview-79411932', './data/interviews/interview-794525', './data/interviews/interview-7959148', './data/interviews/interview-796439', './data/interviews/interview-79667499', './data/interviews/interview-79935965', './data/interviews/interview-7998181', './data/interviews/interview-8095917', './data/interviews/interview-809922', './data/interviews/interview-812725', './data/interviews/interview-81443363', './data/interviews/interview-822576', './data/interviews/interview-8245680', './data/interviews/interview-825165', './data/interviews/interview-82705993', './data/interviews/interview-831512', './data/interviews/interview-833367', './data/interviews/interview-838259', './data/interviews/interview-8387710', './data/interviews/interview-8402388', './data/interviews/interview-8421696', './data/interviews/interview-8464899', './data/interviews/interview-84688694', './data/interviews/interview-849256', './data/interviews/interview-85262552', './data/interviews/interview-8531248', './data/interviews/interview-856221', './data/interviews/interview-8586380', './data/interviews/interview-861780', './data/interviews/interview-862173', './data/interviews/interview-862717', './data/interviews/interview-8631232', './data/interviews/interview-86395001', './data/interviews/interview-865918', './data/interviews/interview-867999', './data/interviews/interview-8700943', './data/interviews/interview-87126591', './data/interviews/interview-871877', './data/interviews/interview-879569', './data/interviews/interview-8819490', './data/interviews/interview-891720', './data/interviews/interview-896668', './data/interviews/interview-9004767', './data/interviews/interview-901603', './data/interviews/interview-901645', './data/interviews/interview-90394637', './data/interviews/interview-904020', './data/interviews/interview-907126', './data/interviews/interview-9074626', './data/interviews/interview-911451', './data/interviews/interview-91673757', './data/interviews/interview-917210', './data/interviews/interview-9185205', './data/interviews/interview-920304', './data/interviews/interview-92391023', './data/interviews/interview-92670500', './data/interviews/interview-927642', './data/interviews/interview-9332386', './data/interviews/interview-9346061', './data/interviews/interview-93473333', './data/interviews/interview-93696502', './data/interviews/interview-938991', './data/interviews/interview-9408565', './data/interviews/interview-94126412', './data/interviews/interview-9437737', './data/interviews/interview-944493', './data/interviews/interview-9446528', './data/interviews/interview-9501580', './data/interviews/interview-95095182', './data/interviews/interview-95601730', './data/interviews/interview-9618669', './data/interviews/interview-9620713', './data/interviews/interview-9651888', './data/interviews/interview-9666149', './data/interviews/interview-97043057', './data/interviews/interview-9709892', './data/interviews/interview-9711852', './data/interviews/interview-9712946', './data/interviews/interview-9728756', './data/interviews/interview-97393699', './data/interviews/interview-97409610', './data/interviews/interview-980963', './data/interviews/interview-982013', './data/interviews/interview-9824821', './data/interviews/interview-98912259', './data/interviews/interview-9901455', './data/interviews/interview-9912172', './data/interviews/interview-992072', './data/interviews/interview-99643550', './data/interviews/interview-9969223', './data/interviews/interview-999372']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import glob\n",
    "\n",
    "files = os.path.join('./data/interviews/', '*')\n",
    "files = glob.glob(files)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected the files neatly, let's comb through them looking for the interviews we want to check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Interview # 47246024 ----\n",
      "Ms. Sun has brown hair and is not from New Zealand.  Not the witness from the cafe.\n",
      "\n",
      "\n",
      "\n",
      "---- Interview # 699607 ----\n",
      "Interviewed Ms. Church at 2:04 pm.  Witness stated that she did not see anyone she could identify as the shooter, that she ran away as soon as the shots were fired.\n",
      "\n",
      "However, she reports seeing the car that fled the scene.  Describes it as a blue Honda, with a license plate that starts with \"L337\" and ends with \"9\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "interview_nums = [\n",
    "    \"47246024\",\n",
    "    \"699607\"]\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "for file in files:\n",
    "    if file.split('-')[1] in interview_nums:\n",
    "        with open(file, 'r') as f:\n",
    "            print(f'---- Interview # {file.split(\"-\")[1]} ----')\n",
    "            print(f.read())\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take note of that information, and check the vehicle database to see if we can find a suspect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Vehicle Database\n",
    "\n",
    "Now that we have some more information, we should be able to put the facts we've figured out thusfar together to get a suspect!\n",
    "\n",
    "Let's start by looking through the vehicle data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>License Plate</th>\n",
       "      <th>Make</th>\n",
       "      <th>Color</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Owner Height</th>\n",
       "      <th>Owner Weight (lbs.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T3YUHF6</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>Jianbo Megannem</td>\n",
       "      <td>5'6\"</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EZ21ECE</td>\n",
       "      <td>BMW</td>\n",
       "      <td>Gold</td>\n",
       "      <td>Norbert Feldwehr</td>\n",
       "      <td>5'3\"</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CQN2TJE</td>\n",
       "      <td>Mazda</td>\n",
       "      <td>Red</td>\n",
       "      <td>Alexandra Jokinen</td>\n",
       "      <td>5'11\"</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D875IMS</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>Orange</td>\n",
       "      <td>Thi Kostadinov</td>\n",
       "      <td>5'11\"</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q0SK1KP</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>Red</td>\n",
       "      <td>Sidni Sze</td>\n",
       "      <td>5'9\"</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  License Plate      Make   Color              Owner Owner Height  \\\n",
       "0       T3YUHF6    Toyota  Yellow    Jianbo Megannem         5'6\"   \n",
       "1       EZ21ECE       BMW    Gold   Norbert Feldwehr         5'3\"   \n",
       "2       CQN2TJE     Mazda     Red  Alexandra Jokinen        5'11\"   \n",
       "3       D875IMS  Cadillac  Orange     Thi Kostadinov        5'11\"   \n",
       "4       Q0SK1KP  Cadillac     Red          Sidni Sze         5'9\"   \n",
       "\n",
       "   Owner Weight (lbs.)  \n",
       "0                  246  \n",
       "1                  205  \n",
       "2                  227  \n",
       "3                  198  \n",
       "4                  199  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "vehicle_dataframe = pd.read_csv('./data/vehicles/vehicles.csv', index_col=0)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "vehicle_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the make and colour the suspect drove while leaving Pandas Express, thanks to the interview, and we also know the suspects height! We also know some information about the suspects license plate! \n",
    "\n",
    "Let's combine all this information and see what we can learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>License Plate</th>\n",
       "      <th>Make</th>\n",
       "      <th>Color</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Owner Height</th>\n",
       "      <th>Owner Weight (lbs.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>L337QE9</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Erika Owens</td>\n",
       "      <td>6'5\"</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>L337DV9</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Joe Germuska</td>\n",
       "      <td>6'2\"</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>L3375A9</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jeremy Bowers</td>\n",
       "      <td>6'1\"</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4901</th>\n",
       "      <td>L337WR9</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jacqui Maher</td>\n",
       "      <td>6'2\"</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     License Plate   Make Color          Owner Owner Height  \\\n",
       "1023       L337QE9  Honda  Blue    Erika Owens         6'5\"   \n",
       "3547       L337DV9  Honda  Blue   Joe Germuska         6'2\"   \n",
       "4546       L3375A9  Honda  Blue  Jeremy Bowers         6'1\"   \n",
       "4901       L337WR9  Honda  Blue   Jacqui Maher         6'2\"   \n",
       "\n",
       "      Owner Weight (lbs.)  \n",
       "1023                  220  \n",
       "3547                  164  \n",
       "4546                  204  \n",
       "4901                  130  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# be sure to include the information in the format provided from the DataFrame above!\n",
    "vehicle_suspects_dataframe = vehicle_dataframe[\n",
    "    (vehicle_dataframe['Make'] == 'Honda') & \n",
    "    (vehicle_dataframe['Color'] == 'Blue') & \n",
    "    (vehicle_dataframe['License Plate'].str.startswith('L337')) &\n",
    "    (vehicle_dataframe['License Plate'].str.endswith('9')) &\n",
    "    (vehicle_dataframe['Owner Height'].str.startswith(\"6'\"))\n",
    "]\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "vehicle_suspects_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have it, a narrowed list of names - let's see if we can fit the final clue into the picture in order to wrap this case up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Information on Memberships\n",
    "\n",
    "We have data from the following memberships on file:\n",
    "\n",
    "1. `AAA`\n",
    "2. `AAdvantage`\n",
    "3. `Costco`\n",
    "4. `Delta_SkyMiles`\n",
    "5. `Fitness_Galaxy`\n",
    "6. `Museum_of_Bash_History`\n",
    "7. `REI`\n",
    "8. `Rotary_Club`\n",
    "9. `TCSU_Alumni_Association`\n",
    "10. `Terminal_City_Library`\n",
    "11. `United_MileagePlus`\n",
    "\n",
    "From the evidence collected, input the memberships that the suspect held."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# Be sure to include all of the suspects memberships in the format provided above!\n",
    "memberships = [\n",
    "    'AAA',\n",
    "    'Delta_SkyMiles',\n",
    "    'Museum_of_Bash_History',\n",
    "    'Terminal_City_Library'\n",
    "]\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "memberships = list(map(lambda x: f\"{x}.csv\", memberships))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using a combination of `glob` and `os` again! Because those libraries should already be imported, we doon't need to import them again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/memberships/AAA.csv', './data/memberships/AAdvantage.csv', './data/memberships/Costco.csv', './data/memberships/Delta_SkyMiles.csv', './data/memberships/Fitness_Galaxy.csv', './data/memberships/Museum_of_Bash_History.csv', './data/memberships/REI.csv', './data/memberships/Rotary_Club.csv', './data/memberships/TCSU_Alumni_Association.csv', './data/memberships/Terminal_City_Library.csv', './data/memberships/United_MileagePlus.csv']\n"
     ]
    }
   ],
   "source": [
    "files = os.path.join('./data/memberships/', '*.csv')\n",
    "files = glob.glob(files)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll combine these `.csv` files together into a list of `pandas.DataFrame`s (read more about those [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "membership_dataframe_list = [pd.read_csv(file) for file in files if file.endswith(tuple(memberships))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll find the intersection of the memberships using `pandas.merge` (read about it [here](https://pandas.pydata.org/docs/reference/api/pandas.merge.html)) on the list of dataframes.\n",
    "\n",
    "We're using `reduce` from functools (read about it [here](https://www.geeksforgeeks.org/reduce-in-python/)) to perform the `pandas.merge` operation on a number of `pandas.DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "membership_suspects_dataframe = reduce(lambda x, y: pd.merge(x, y, how='inner'), membership_dataframe_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Alright, now that we have our vehicle suspects, our membership suspects, as well as the database of people - let's put this all together and see what we can find!\n",
    "\n",
    "First step is to merge the two suspect lists to narrow it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>License Plate</th>\n",
       "      <th>Make</th>\n",
       "      <th>Color</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Owner Height</th>\n",
       "      <th>Owner Weight (lbs.)</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L3375A9</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jeremy Bowers</td>\n",
       "      <td>6'1\"</td>\n",
       "      <td>204</td>\n",
       "      <td>Jeremy Bowers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L337WR9</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jacqui Maher</td>\n",
       "      <td>6'2\"</td>\n",
       "      <td>130</td>\n",
       "      <td>Jacqui Maher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  License Plate   Make Color          Owner Owner Height  Owner Weight (lbs.)  \\\n",
       "0       L3375A9  Honda  Blue  Jeremy Bowers         6'1\"                  204   \n",
       "1       L337WR9  Honda  Blue   Jacqui Maher         6'2\"                  130   \n",
       "\n",
       "           names  \n",
       "0  Jeremy Bowers  \n",
       "1   Jacqui Maher  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_suspect_dataframe = pd.merge(vehicle_suspects_dataframe, membership_suspects_dataframe, left_on='Owner', right_on='names')\n",
    "merged_suspect_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gah! We have two suspects, let's check `people_dataframe` and see if there's anyway we can decide! \n",
    "\n",
    "HINT: Look at the new columns added as a result of the merge and remember what we know about our suspect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>License Plate</th>\n",
       "      <th>Make</th>\n",
       "      <th>Color</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Owner Height</th>\n",
       "      <th>Owner Weight (lbs.)</th>\n",
       "      <th>names</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L3375A9</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jeremy Bowers</td>\n",
       "      <td>6'1\"</td>\n",
       "      <td>204</td>\n",
       "      <td>Jeremy Bowers</td>\n",
       "      <td>Jeremy Bowers</td>\n",
       "      <td>M</td>\n",
       "      <td>34</td>\n",
       "      <td>Dunstable Road, line 284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L337WR9</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jacqui Maher</td>\n",
       "      <td>6'2\"</td>\n",
       "      <td>130</td>\n",
       "      <td>Jacqui Maher</td>\n",
       "      <td>Jacqui Maher</td>\n",
       "      <td>F</td>\n",
       "      <td>40</td>\n",
       "      <td>Andover Road, line 224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  License Plate   Make Color          Owner Owner Height  Owner Weight (lbs.)  \\\n",
       "0       L3375A9  Honda  Blue  Jeremy Bowers         6'1\"                  204   \n",
       "1       L337WR9  Honda  Blue   Jacqui Maher         6'2\"                  130   \n",
       "\n",
       "           names           name gender  age                   address  \n",
       "0  Jeremy Bowers  Jeremy Bowers      M   34  Dunstable Road, line 284  \n",
       "1   Jacqui Maher   Jacqui Maher      F   40    Andover Road, line 224  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_dataframe = pd.merge(merged_suspect_dataframe, people_dataframe, left_on=\"Owner\", right_on='name')\n",
    "final_merged_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: Look at the new columns added as a result of the merge and remember what we know about our suspect! Pay close attention to the `gender` column!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our final suspect, let's see if it's correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You have officially solved the crime at the Pandas Express! Thanks for participating!\n",
      "And now that you've completed your ML software development onramp, tell the world that you're ready to begin your journey! \n",
      "\n",
      "---------------------------------------- Post Template 1 (MLE) ----------------------------------------------------\n",
      "I just completed the #ml software development onramp from @FourthBrain and solved a #whodunit mystery!\n",
      "Now I'm ready to take my #ml career to the next level with an industry-standard tool stack, including the Unix Command Line, Git, Conda, Pip, and of course, Jupyter Notebooks!\n",
      "#machinelearningengineering #mle #buildinpublic\n",
      "--------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------- Post Template 2 (MLOps) ----------------------------------------------------\n",
      "I just completed the #ml software development onramp from @FourthBrain and solved a #whodunit mystery!\n",
      "Now Im ready to take my #ml career to the next level with an industry-standard tool stack including the Unix Command Line, Git, Conda, Pip, and Jupyter Notebooks, optimized for agile development in VSCode.\n",
      "#machinelearningoperations #mlops #buildinpublic\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "As a bonus - Please checkout the main branch and execute this command in your command line: `git cat-file -p cefafaedfff0fc1e3495252571f48bf7b0d93673`\n"
     ]
    }
   ],
   "source": [
    "from helper_functions.check_name import check_name\n",
    "\n",
    "check_name('Jeremy Bowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "635e31ff34c0350df6e9d804eda70786d94f48b17fcc73c378df4ea6ec0d01fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
